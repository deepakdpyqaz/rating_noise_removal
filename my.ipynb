{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skfuzzy as fuzz\n",
    "from surprise import reader\n",
    "from surprise.prediction_algorithms import SlopeOne\n",
    "from surprise import Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ml100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"timestamp\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rating=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = np.arange(1, max_rating + 1, 1)\n",
    "\n",
    "diff = (max_rating - 1) / 2\n",
    "\n",
    "rating_low = fuzz.membership.gaussmf(rating, 1, diff)\n",
    "rating_mid = fuzz.membership.gaussmf(rating, 1 + diff, diff)\n",
    "rating_high = fuzz.membership.gaussmf(rating, 1 + 2 * diff, diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"r_low\"] = df[\"rating\"].apply(\n",
    "    lambda x: fuzz.interp_membership(rating, rating_low, x)\n",
    ")\n",
    "df[\"r_mid\"] = df[\"rating\"].apply(\n",
    "    lambda x: fuzz.interp_membership(rating, rating_mid, x)\n",
    ")\n",
    "df[\"r_high\"] = df[\"rating\"].apply(\n",
    "    lambda x: fuzz.interp_membership(rating, rating_high, x)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = df[[\"user\", \"r_low\", \"r_mid\", \"r_high\"]].groupby(\"user\").mean()\n",
    "df_item = df[[\"item\", \"r_low\", \"r_mid\", \"r_high\"]].groupby(\"item\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimated_ratings(row):\n",
    "    user = row[\"user\"]\n",
    "    item = row[\"item\"]\n",
    "    current_rating = row[\"rating\"]\n",
    "\n",
    "    r_low = row[\"r_low\"]\n",
    "    r_mid = row[\"r_mid\"]\n",
    "    r_high = row[\"r_high\"]\n",
    "\n",
    "    user_low = df_user.loc[user][\"r_low\"]\n",
    "    user_mid = df_user.loc[user][\"r_mid\"]\n",
    "    user_high = df_user.loc[user][\"r_high\"]\n",
    "    # print(user_low, user_mid, user_high)\n",
    "    item_low = df_item.loc[item][\"r_low\"]\n",
    "    item_mid = df_item.loc[item][\"r_mid\"]\n",
    "    item_high = df_item.loc[item][\"r_high\"]\n",
    "    # print(item_low, item_mid, item_high)\n",
    "\n",
    "    estimated_low = (user_low * item_low)\n",
    "    estimated_mid = (user_mid * item_mid)\n",
    "    estimated_high = (user_high * item_high)\n",
    "    total = estimated_low + estimated_mid + estimated_high\n",
    "    estimated_low = estimated_low / total\n",
    "    estimated_mid = estimated_mid / total\n",
    "    estimated_high = estimated_high / total\n",
    "    \n",
    "    # print(estimated_low, estimated_mid, estimated_high)\n",
    "    r_v = np.array([r_low, r_mid, r_high])\n",
    "    e_v = np.array([estimated_low, estimated_mid, estimated_high])\n",
    "    dist = np.linalg.norm(r_v - e_v)\n",
    "    s = 1/(1+dist)\n",
    "    # print(s)\n",
    "    if s < 0.8:\n",
    "        e_low = np.fmin(estimated_low,rating_low)\n",
    "        e_mid = np.fmin(estimated_mid,rating_mid)\n",
    "        e_high = np.fmin(estimated_high,rating_high)\n",
    "\n",
    "        aggregated = np.fmax(e_low, np.fmax(e_mid, e_high))\n",
    "        estimated = fuzz.defuzz(rating, aggregated, \"mom\")\n",
    "        return estimated\n",
    "    else:\n",
    "        return current_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user         301.000000\n",
       "item         401.000000\n",
       "rating         4.000000\n",
       "r_low          0.324652\n",
       "r_mid          0.882497\n",
       "r_high         0.882497\n",
       "estimated      3.000000\n",
       "Name: 250, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_estimated_ratings(df.iloc[250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"estimated\"] = df.apply(get_estimated_ratings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100000.000000\n",
       "mean          3.128390\n",
       "std           0.347688\n",
       "min           1.500000\n",
       "25%           3.000000\n",
       "50%           3.000000\n",
       "75%           3.000000\n",
       "max           4.500000\n",
       "Name: estimated, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.estimated.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "round = df.estimated.apply(round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    86358\n",
       "4    13254\n",
       "2      388\n",
       "Name: estimated, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    34174\n",
       "3    27145\n",
       "5    21201\n",
       "2    11370\n",
       "1     6110\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[[\"user\",\"item\",\"rating\",\"estimated\"]].to_csv(\"ml100k_corrected_my.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import SlopeOne, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse, mae\n",
    "from surprise.reader import Reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(algo, trainset, testset):\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    rmse_error = rmse(predictions)\n",
    "    mae_error = mae(predictions)\n",
    "    print(f\"For {algo} RMSE is {rmse_error} and MAE is {mae_error}\")\n",
    "    return algo, rmse_error, mae_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algos(trainset,testset):\n",
    "    algorithms = [\n",
    "        SlopeOne(),\n",
    "        # KNNBasic(k=60, sim_options={\"name\": \"pearson\", \"user_based\": True}),\n",
    "        # KNNBasic(k=60, sim_options={\"name\": \"pearson\", \"user_based\": False}),\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for algo in algorithms:\n",
    "        results.append(get_accuracy(algo, trainset, testset))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  80000\n",
      "Test set size:  20000\n"
     ]
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(\"Training set size: \", trainset.n_ratings)\n",
    "print(\"Test set size: \", len(df) - trainset.n_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9423\n",
      "MAE:  0.7414\n",
      "For <surprise.prediction_algorithms.slope_one.SlopeOne object at 0x000001FAD1C49E50> RMSE is 0.9422541799329658 and MAE is 0.7413586954857572\n"
     ]
    }
   ],
   "source": [
    "results_base = run_algos(trainset,testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(estimated=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  80000\n",
      "Test set size:  20000\n"
     ]
    }
   ],
   "source": [
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"estimated\"]], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(\"Training set size: \", trainset.n_ratings)\n",
    "print(\"Test set size: \", len(df) - trainset.n_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0000\n",
      "MAE:  0.0000\n",
      "For <surprise.prediction_algorithms.slope_one.SlopeOne object at 0x000001FAD8364F40> RMSE is 0.0 and MAE is 0.0\n"
     ]
    }
   ],
   "source": [
    "results_estimated = run_algos(trainset,testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "wang = pd.read_csv(\"ml100k_corrected_wang.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    49869\n",
       "4.0    45025\n",
       "5.0     3585\n",
       "2.0     1226\n",
       "1.0      295\n",
       "Name: corrected_rating, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wang.corrected_rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    34174\n",
       "3    27145\n",
       "5    21201\n",
       "2    11370\n",
       "1     6110\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wang.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.n_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = ts.build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctsd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
